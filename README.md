
# Bike Sharing Data Science Project 

This notebook documents the analysis and model development for the Bike Sharing Dataset. It contains the following steps:

- About the Bike Sharing Dataset
- Descriptive Analysis
- Missing Value Analysis
- Outlier Analysis
- Correlation Analysis
- Overview metrics
- Model Selection
    -  Ridge Regession
    - Support Vector Regression
    - Essemble Regressor
    - Random Forest Regressor
- Random Forest
    - Random Forest Model
    - Feature importance
- Conclusion
- Future Work

## About the Bike Sharing Dataset

### Overview

Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. 

Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.

### Attribute Information

Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv

- instant: record index
- dteday : date
- season : season (1:springer, 2:summer, 3:fall, 4:winter)
- yr : year (0: 2011, 1:2012)
- mnth : month ( 1 to 12)
- hr : hour (0 to 23)
- holiday : weather day is holiday or not 
- weekday : day of the week
- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
+ weathersit : 
    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
    - temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)
    - atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
- hum: Normalized humidity. The values are divided to 100 (max)
- windspeed: Normalized wind speed. The values are divided to 67 (max)
- casual: count of casual users
- registered: count of registered users
- cnt: count of total rental bikes including both casual and registered

## Descriptive Analysis


```python
from dataloader import Dataloader
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import seaborn as sns
import matplotlib.pyplot as plt
from prettytable import PrettyTable
import numpy as np
# Sklearn metrics
from sklearn.metrics import mean_squared_error, mean_squared_log_error
# Sklearn models
from sklearn.linear_model import Lasso, ElasticNet, Ridge, SGDRegressor
from sklearn.svm import SVR, NuSVR
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.linear_model import LinearRegression

%matplotlib inline
init_notebook_mode(connected=True)
```


<script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type="text/javascript">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>


Provide data set splits for training, validation and testing:


```python
dataloader = Dataloader('Bike-Sharing-Dataset/hour.csv')
train, val, test = dataloader.getData()
fullData = dataloader.getFullData()

features= ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']
target = ['cnt']
```

Get column names of pandas data frame:


```python
print(list(fullData.columns))
```

    ['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']
    

Print the first two samples of the dataset to explore the data:


```python
print(fullData.head(2))
```

       instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \
    0        1  2011-01-01       1   0     1   0        0        6           0   
    1        2  2011-01-01       1   0     1   1        0        6           0   
    
       weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  
    0           1  0.24  0.2879  0.81        0.0       3          13   16  
    1           1  0.22  0.2727  0.80        0.0       8          32   40  
    

Get data statistics for each column:


```python
print(fullData.describe())
```

              instant        season            yr          mnth            hr  \
    count  17379.0000  17379.000000  17379.000000  17379.000000  17379.000000   
    mean    8690.0000      2.501640      0.502561      6.537775     11.546752   
    std     5017.0295      1.106918      0.500008      3.438776      6.914405   
    min        1.0000      1.000000      0.000000      1.000000      0.000000   
    25%     4345.5000      2.000000      0.000000      4.000000      6.000000   
    50%     8690.0000      3.000000      1.000000      7.000000     12.000000   
    75%    13034.5000      3.000000      1.000000     10.000000     18.000000   
    max    17379.0000      4.000000      1.000000     12.000000     23.000000   
    
                holiday       weekday    workingday    weathersit          temp  \
    count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   
    mean       0.028770      3.003683      0.682721      1.425283      0.496987   
    std        0.167165      2.005771      0.465431      0.639357      0.192556   
    min        0.000000      0.000000      0.000000      1.000000      0.020000   
    25%        0.000000      1.000000      0.000000      1.000000      0.340000   
    50%        0.000000      3.000000      1.000000      1.000000      0.500000   
    75%        0.000000      5.000000      1.000000      2.000000      0.660000   
    max        1.000000      6.000000      1.000000      4.000000      1.000000   
    
                  atemp           hum     windspeed        casual    registered  \
    count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   
    mean       0.475775      0.627229      0.190098     35.676218    153.786869   
    std        0.171850      0.192930      0.122340     49.305030    151.357286   
    min        0.000000      0.000000      0.000000      0.000000      0.000000   
    25%        0.333300      0.480000      0.104500      4.000000     34.000000   
    50%        0.484800      0.630000      0.194000     17.000000    115.000000   
    75%        0.621200      0.780000      0.253700     48.000000    220.000000   
    max        1.000000      1.000000      0.850700    367.000000    886.000000   
    
                    cnt  
    count  17379.000000  
    mean     189.463088  
    std      181.387599  
    min        1.000000  
    25%       40.000000  
    50%      142.000000  
    75%      281.000000  
    max      977.000000  
    

## Missing Value Analysis

Check any NULL values in data:


```python
print(fullData.isnull().any())
```

    instant       False
    dteday        False
    season        False
    yr            False
    mnth          False
    hr            False
    holiday       False
    weekday       False
    workingday    False
    weathersit    False
    temp          False
    atemp         False
    hum           False
    windspeed     False
    casual        False
    registered    False
    cnt           False
    dtype: bool
    

## Outlier Analysis

### Box plots


```python
fig, axes = plt.subplots(nrows=3,ncols=2)
fig.set_size_inches(15, 15)
sns.boxplot(data=train,y="cnt",orient="v",ax=axes[0][0])
sns.boxplot(data=train,y="cnt",x="mnth",orient="v",ax=axes[0][1])
sns.boxplot(data=train,y="cnt",x="weathersit",orient="v",ax=axes[1][0])
sns.boxplot(data=train,y="cnt",x="workingday",orient="v",ax=axes[1][1])
sns.boxplot(data=train,y="cnt",x="hr",orient="v",ax=axes[2][0])
sns.boxplot(data=train,y="cnt",x="temp",orient="v",ax=axes[2][1])

axes[0][0].set(ylabel='Count',title="Box Plot On Count")
axes[0][1].set(xlabel='Month', ylabel='Count',title="Box Plot On Count Across Months")
axes[1][0].set(xlabel='Weather Situation', ylabel='Count',title="Box Plot On Count Across Weather Situations")
axes[1][1].set(xlabel='Working Day', ylabel='Count',title="Box Plot On Count Across Working Day")
axes[2][0].set(xlabel='Hour Of The Day', ylabel='Count',title="Box Plot On Count Across Hour Of The Day")
axes[2][1].set(xlabel='Temperature', ylabel='Count',title="Box Plot On Count Across Temperature")

```




    [Text(0, 0.5, 'Count'),
     Text(0.5, 0, 'Temperature'),
     Text(0.5, 1.0, 'Box Plot On Count Across Temperature')]




![output_22_1](https://user-images.githubusercontent.com/6838540/53189989-fa63e600-3608-11e9-9469-fa93e43ad7a9.png)


__Interpretation:__ The working day and holiday box plots indicate that more bicycles are rent during normal working days than on weekends or holidays. The hourly box plots show a local maximum at 8 am and one at 5 pm which indicates that most users of the bicycle rental service use the bikes to get to work or school. Another important factor seems to be the temperature: higher temperatures lead to an increasing number of bike rents and lower temperatures not only decrease the average number of rents but also shows more outliers in the data.

### Remove outliers from data

 Keep only the ones that are within +3 to -3 standard deviations in the column 'cnt':


```python
print("Samples in train set with outliers: {}".format(len(train)))
train_preprocessed = train[np.abs(train['cnt']-train['cnt'].mean()) <= (3*train['cnt'].std())]
print("Samples in train set without outliers: {}".format(len(train_preprocessed)))
```

    Samples in train set with outliers: 15641
    Samples in train set without outliers: 15407
    

## Correlation Analysis


```python
matrix = train[features + target].corr()
heat = np.array(matrix)
heat[np.tril_indices_from(heat)] = False
fig,ax= plt.subplots()
fig.set_size_inches(20,10)
sns.heatmap(matrix, mask=heat,vmax=1.0, vmin=0.0, square=True,annot=True, cmap="Reds")
```




    <matplotlib.axes._subplots.AxesSubplot at 0x20f2fa6c160>




![output_28_1](https://user-images.githubusercontent.com/6838540/53190016-0c458900-3609-11e9-92be-bc8edab17cab.png)


__Interpretation:__ The correlation matrix of the different attributes reveals the following points:

- Casual and registred contain direct information about the bike sharing count which is to predict (data leakage). Therefore they are not considered in the feature set (and removed from the plot).
- The variables "temp" and "atemp" are strongly correlated. To reduce the dimensionality of the predictive model, the feature "atemp" is dismissed.
- The variables "hr" and "temp" seem to be promissing features for the bike sharing count prediction. The variables "holiday", "weekend", "workingday", "weathersit" and "windspeed" do not contribute a lort of information.



```python
features.remove('atemp')
```

## Overview Metrics

### Mean Squared Error (MSE)

MSE = $\sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - y_i)^2}$

### Root Mean Squared Logarithmic Error (RMSLE)

RMSLE = $\sqrt{ \frac{1}{N} \sum_{i=1}^N (\log(x_i) - \log(y_i))^2 }$

### $R^2$ Score

$R^2=1-\frac{\sum_{i=1}^{n}e_i^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}$

## Model Selection

The characteristics of the given problem are:

- __Regression:__ The target variable is a quantity.
- __Small dataset:__ Less than 100K samples.
- __Few features should be important:__ The correlation matrix indicates that a few features contain the information to predict the target variable.

This characteristics makes the follwoing methods most promissing: Ridge Regession, Support Vector Regression, Essemble Regressor, Random Forest Regressor.

We will evaluate the performance of these models in the follwing:


```python
x_train = train_preprocessed[features].values
y_train = train_preprocessed[target].values.ravel()
# Sort validation set for plots
val = val.sort_values(by=target)
x_val = val[features].values
y_val = val[target].values.ravel()
x_test = test[features].values

table = PrettyTable()
table.field_names = ["Model", "Mean Squared Error", "R² score"]

models = [
    SGDRegressor(max_iter=1000, tol=1e-3),
    Lasso(alpha=0.1),
    ElasticNet(random_state=0),
    Ridge(alpha=.5),
    SVR(gamma='auto', kernel='linear'),
    SVR(gamma='auto', kernel='rbf'),
    BaggingRegressor(),
    BaggingRegressor(KNeighborsClassifier(), max_samples=0.5, max_features=0.5),
    BaggingRegressor(KMeans(), max_samples=0.5, max_features=0.5),
    NuSVR(gamma='auto'),
    RandomForestRegressor( random_state=0, n_estimators=300)
]

for model in models:
    model.fit(x_train, y_train) 
    y_res = model.predict(x_val)

    mse = mean_squared_error(y_val, y_res)
    score = model.score(x_val, y_val)    

    table.add_row([type(model).__name__, format(mse, '.2f'), format(score, '.2f')])

print(table)
```

    +-----------------------+--------------------+----------+
    |         Model         | Mean Squared Error | R² score |
    +-----------------------+--------------------+----------+
    |      SGDRegressor     |      31849.71      |   0.22   |
    |         Lasso         |      34518.80      |   0.15   |
    |       ElasticNet      |      35014.60      |   0.14   |
    |         Ridge         |      34523.55      |   0.15   |
    |          SVR          |      39555.90      |   0.03   |
    |          SVR          |      29410.76      |   0.28   |
    |    BaggingRegressor   |      11883.09      |   0.71   |
    |    BaggingRegressor   |      27923.97      |   0.32   |
    |    BaggingRegressor   |      93807.30      |  -1.30   |
    |         NuSVR         |      28148.48      |   0.31   |
    | RandomForestRegressor |      12276.23      |   0.70   |
    +-----------------------+--------------------+----------+
    

## Random Forest

### Random Forest Model


```python
# Table setup
table = PrettyTable()
table.field_names = ["Model", "Dataset", "MSE", 'RMSLE', "R² score"]
# Model training
model = RandomForestRegressor( random_state=0, n_estimators=100)
model.fit(x_train, y_train) 

def evaluate(x, y, dataset):
    pred = model.predict(x)

    mse = mean_squared_error(y, pred)
    score = model.score(x, y)    
    rmsle = np.sqrt(mean_squared_log_error(y, pred))

    table.add_row([type(model).__name__, dataset, format(mse, '.2f'), format(rmsle, '.2f'), format(score, '.2f')])
    

evaluate(x_train, y_train, 'training')
evaluate(x_val, y_val, 'validation')

print(table)
```

    +-----------------------+------------+----------+-------+----------+
    |         Model         |  Dataset   |   MSE    | RMSLE | R² score |
    +-----------------------+------------+----------+-------+----------+
    | RandomForestRegressor |  training  |  501.19  |  0.18 |   0.98   |
    | RandomForestRegressor | validation | 12221.83 |  0.47 |   0.70   |
    +-----------------------+------------+----------+-------+----------+
    

### Feature importance


```python
importances = model.feature_importances_
std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
indices = np.argsort(importances)[::-1]
```


```python
# Print the feature ranking
print("Feature ranking:")

for f in range(x_val.shape[1]):
    print("%d. feature %s (%f)" % (f + 1, features[indices[f]], importances[indices[f]]))
```

    Feature ranking:
    1. feature hr (0.596542)
    2. feature temp (0.180670)
    3. feature hum (0.062546)
    4. feature workingday (0.046904)
    5. feature windspeed (0.033084)
    6. feature mnth (0.025973)
    7. feature weathersit (0.021608)
    8. feature weekday (0.020289)
    9. feature season (0.009326)
    10. feature holiday (0.003057)
    


```python
# Plot the feature importances of the forest
plt.figure(figsize=(18,5))
plt.title("Feature importances")
plt.bar(range(x_val.shape[1]), importances[indices], color="cornflowerblue", yerr=std[indices], align="center")
plt.xticks(range(x_val.shape[1]), [features[i] for i in indices])
plt.xlim([-1, x_val.shape[1]])
plt.show()
```


![output_47_0](https://user-images.githubusercontent.com/6838540/53190023-1071a680-3609-11e9-880b-db695124cdf5.png)



__Interpretation:__ The result corresponds to the high correlation of the hour and temerature variable with the bycicle sharing count in the feature correlation matrix.

## Future Work

Here are some ideas of future work to improve the performance of the data model further:

- Distribution adjustment of the target variable: Some predictive models assume a normal distribution of the target variable - a transformation in the data preprocessing could improve the performance of such methods. 
- Large scale dataset implementation of random forests. For large scale datasets (> 10 Mio. samples) the used sklearn python implementation of random forests will exteremly slow down if it is unabble to hold all samples in the working maemory or can run into serious memory problems. A solution could be the [woody](https://github.com/gieseke/woody) implementation with top trees for pre classification and flat random forests implemented in C at the leaves of the top trees.
